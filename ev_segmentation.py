# -*- coding: utf-8 -*-
"""EV_segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f5RFFgEV6m_tLIqB-PCezFDhy3AX54LH
"""

# ev_fleet_segmentation_adaptive.py
"""
Adaptive EV Fleet segmentation script.
- Auto-detects local dataset files and column names (with fuzzy matching).
- Produces EDA charts, KMeans clustering, cluster profiling, folium map.
- Saves outputs to ./outputs and writes a load_log.txt describing datasets used.
"""

import os, sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import folium
from difflib import get_close_matches

# Optional: try Levenshtein to improve fuzzy matching; not required
try:
    import Levenshtein  # type: ignore
    have_lev = True
except Exception:
    have_lev = False

# -------------------------
# User-editable filenames
# -------------------------
FLEET_FILES = ["fleet_telemetry.csv", "fleet_telemetry_sample.csv", "fleet_with_clusters.csv", "fleet.csv"]
CHARGING_FILES = ["charging_stations_india.csv", "charging_stations.csv"]
SALES_FILES = ["indian_2w_ev_sales.csv", "2w_sales.csv"]

OUT_DIR = "outputs"
os.makedirs(OUT_DIR, exist_ok=True)
log_lines = []

# -------------------------
# Helper: fuzzy column mapper
# -------------------------
def fuzzy_map_column(col_candidates, desired_names):
    """
    col_candidates: list of actual column names in DF
    desired_names: list of possible synonyms, ordered by priority
    returns matched candidate or None
    """
    # exact match by candidate names
    for name in desired_names:
        if name in col_candidates:
            return name
    # case-insensitive exact
    low_map = {c.lower(): c for c in col_candidates}
    for name in desired_names:
        if name.lower() in low_map:
            return low_map[name.lower()]
    # fuzzy via difflib
    for name in desired_names:
        matches = get_close_matches(name, col_candidates, n=1, cutoff=0.75)
        if matches:
            return matches[0]
    # optional Levenshtein-based best ratio
    if have_lev:
        best=None; best_score=0
        for c in col_candidates:
            for name in desired_names:
                s = Levenshtein.ratio(c.lower(), name.lower())
                if s>best_score:
                    best_score=s; best=c
        if best_score>0.6:
            return best
    return None

# -------------------------
# Step 1: load files if present
# -------------------------
def find_file(candidates):
    for f in candidates:
        if os.path.exists(f):
            return f
    return None

fleet_file = find_file(FLEET_FILES)
charging_file = find_file(CHARGING_FILES)
sales_file = find_file(SALES_FILES)

if fleet_file:
    try:
        fleet_df = pd.read_csv(fleet_file)
        log_lines.append(f"Loaded fleet file: {fleet_file} with shape {fleet_df.shape}")
    except Exception as e:
        log_lines.append(f"Failed to read fleet file {fleet_file}: {e}")
        fleet_file=None
else:
    log_lines.append("No fleet file found among candidates; will generate synthetic fleet dataset.")

if charging_file:
    try:
        charging_df = pd.read_csv(charging_file)
        log_lines.append(f"Loaded charging file: {charging_file} with shape {charging_df.shape}")
    except Exception as e:
        log_lines.append(f"Failed to read charging file {charging_file}: {e}")
        charging_file=None
else:
    charging_df=None
    log_lines.append("No charging stations file found.")

if sales_file:
    try:
        sales_df = pd.read_csv(sales_file)
        log_lines.append(f"Loaded 2W sales file: {sales_file} with shape {sales_df.shape}")
    except Exception as e:
        log_lines.append(f"Failed to read sales file {sales_file}: {e}")
        sales_file=None
else:
    sales_df=None
    log_lines.append("No 2W sales file found.")

# -------------------------
# Fallback synthetic generator
# -------------------------
def generate_synthetic_fleet(n=200, seed=42):
    np.random.seed(seed)
    df = pd.DataFrame({
        "fleet_id": [f"F{1000+i}" for i in range(n)],
        "avg_km_day": np.clip(np.random.normal(loc=80, scale=18, size=n).astype(int), 20, 200),
        "fleet_size": np.random.choice([5,8,10,12,15,20,25,30,40,50], n, p=[0.1,0.08,0.12,0.12,0.12,0.15,0.12,0.08,0.06,0.05]),
        "downtime_tol_min": np.random.randint(5, 45, n),
        "pct_depot_charging": np.clip(np.random.normal(60, 30, n).astype(int), 0, 100),
        "zone": np.random.choice(["Gachibowli","Madhapur","Banjara Hills","Abids","Kukatpally","Other"], n, p=[0.18,0.2,0.12,0.15,0.15,0.2])
    })
    df["energy_cost_inr_per_km"] = np.round(0.16 + np.random.normal(0.02, 0.03, n), 3)
    df["capex_est_inr"] = np.where(df["fleet_size"]<15, 90000, 80000)
    df["monthly_tco_per_vehicle_inr"] = np.round(df["capex_est_inr"]/36 + df["avg_km_day"]*30*df["energy_cost_inr_per_km"] + 500, 0)
    return df

if fleet_file is None:
    fleet_df = generate_synthetic_fleet(n=250)
    log_lines.append("Generated synthetic fleet dataset (n=250).")

# -------------------------
# Step 2: detect & map columns
# -------------------------
orig_cols = list(fleet_df.columns)
log_lines.append(f"Fleet dataset columns: {orig_cols}")

# desired features and synonyms
desired_map = {
    "avg_km_day": ["avg_km_day","avg_km_per_day","daily_km","km_per_day","avg_distance_day","avg_km"],
    "fleet_size": ["fleet_size","num_vehicles","size","vehicles","fleet_count","num_veh"],
    "downtime_tol_min": ["downtime_tol_min","downtime_minutes","downtime","downtime_min","tolerance_min"],
    "pct_depot_charging": ["pct_depot_charging","depot_charging_pct","depot_pct","percent_depot_charging","pct_depot"],
    "zone": ["zone","area","location","operational_zone","city_zone"],
    "fleet_id": ["fleet_id","id","fleet"]
}

mapped = {}
for target, synonyms in desired_map.items():
    match = fuzzy_map_column(orig_cols, synonyms)
    mapped[target] = match
    log_lines.append(f"Mapping for {target}: {match}")

# For numeric columns we can try casting
# If any required mapping fails, attempt to infer reasonable alternatives (e.g., if 'daily_km' not found but 'distance' exists)
# Provide defaults where necessary
if mapped["avg_km_day"] is None:
    # try to find any numeric column with plausible name
    for c in orig_cols:
        if fleet_df[c].dtype.kind in 'iuf' and fleet_df[c].mean()>10 and fleet_df[c].mean()<200:
            mapped["avg_km_day"] = c
            log_lines.append(f"Auto-chose {c} for avg_km_day (numeric heuristic).")
            break

if mapped["fleet_size"] is None:
    for c in orig_cols:
        if fleet_df[c].dtype.kind in 'iuf' and fleet_df[c].max()<=1000 and fleet_df[c].mean()>1:
            # prefer integer columns
            mapped["fleet_size"] = c
            log_lines.append(f"Auto-chose {c} for fleet_size (numeric heuristic).")
            break

# If still missing, create placeholders (but clustering requires at least avg_km_day and fleet_size)
if mapped["avg_km_day"] is None or mapped["fleet_size"] is None:
    log_lines.append("Important columns missing -> regenerating synthetic dataset to ensure pipeline runs.")
    fleet_df = generate_synthetic_fleet(n=250)
    orig_cols = list(fleet_df.columns)
    for target, synonyms in desired_map.items():
        match = fuzzy_map_column(orig_cols, synonyms)
        mapped[target] = match
    log_lines.append("Regenerated synthetic dataset and remapped columns.")

# For charging station coords mapping
charging_has_coords = False
if charging_df is not None:
    charging_cols = list(charging_df.columns)
    lat_col = fuzzy_map_column(charging_cols, ["latitude","lat","Latitude","LAT"])
    lon_col = fuzzy_map_column(charging_cols, ["longitude","lon","lng","Longitude","LON"])
    if lat_col and lon_col:
        charging_has_coords = True
        mapped_charging = {"lat": lat_col, "lon": lon_col}
        log_lines.append(f"Charging lat/lon mapped to: {mapped_charging}")
    else:
        log_lines.append("Charging file present but lat/lon columns not found; skipping spatial scatter.")

# Save load log
with open(os.path.join(OUT_DIR, "load_log.txt"), "w") as f:
    for L in log_lines:
        f.write(L+"\n")

print("\n".join(log_lines))

# -------------------------
# Step 3: basic EDA plots (using mapped columns)
# -------------------------
sns.set(style="whitegrid", context="talk")
avg_km_col = mapped["avg_km_day"]
fleet_size_col = mapped["fleet_size"]
downtime_col = mapped["downtime_tol_min"]
pct_depot_col = mapped["pct_depot_charging"]
zone_col = mapped["zone"]
fleet_id_col = mapped["fleet_id"] or "fleet_id"

# Ensure numeric types
for c in [avg_km_col, fleet_size_col, downtime_col, pct_depot_col]:
    if c and c in fleet_df.columns:
        fleet_df[c] = pd.to_numeric(fleet_df[c], errors="coerce")

# Basic histograms
plt.figure(figsize=(8,4))
fleet_df[avg_km_col].hist(bins=25)
plt.title("Average km/day distribution")
plt.xlabel("avg_km_day")
plt.savefig(os.path.join(OUT_DIR, "hist_avg_km_day_adaptive.png"), dpi=150)
plt.close()

plt.figure(figsize=(8,4))
fleet_df[fleet_size_col].hist(bins=20)
plt.title("Fleet size distribution")
plt.xlabel("fleet_size")
plt.savefig(os.path.join(OUT_DIR, "hist_fleet_size_adaptive.png"), dpi=150)
plt.close()

if zone_col in fleet_df.columns:
    plt.figure(figsize=(8,4))
    order = fleet_df[zone_col].value_counts().index
    sns.countplot(y=zone_col, data=fleet_df, order=order)
    plt.title("Fleet counts by zone")
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "zone_counts_adaptive.png"), dpi=150)
    plt.close()

# Charging scatter (if coords available)
if charging_df is not None and charging_has_coords:
    plt.figure(figsize=(6,6))
    plt.scatter(charging_df[mapped_charging['lon']], charging_df[mapped_charging['lat']], s=8, alpha=0.6)
    plt.title("Charging station locations (loaded)")
    plt.xlabel("lon"); plt.ylabel("lat")
    plt.savefig(os.path.join(OUT_DIR, "charging_scatter_adaptive.png"), dpi=150)
    plt.close()

# -------------------------
# Step 4: clustering (features chosen)
# -------------------------
features = []
if avg_km_col: features.append(avg_km_col)
if fleet_size_col: features.append(fleet_size_col)
if downtime_col: features.append(downtime_col)
if pct_depot_col: features.append(pct_depot_col)

# Minimal features required: avg_km and fleet_size
if len(features) < 2:
    print("Not enough features for clustering (need at least 2); aborting clustering step.")
else:
    X = fleet_df[features].copy().fillna(fleet_df[features].median())
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X)

    # silhouette check to choose k
    sil_scores = {}
    for k in range(2,6):
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = km.fit_predict(Xs)
        sil = silhouette_score(Xs, labels)
        sil_scores[k] = sil
    best_k = max(sil_scores, key=sil_scores.get)
    print("Silhouette scores:", sil_scores, " -> best k:", best_k)

    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    fleet_df['cluster'] = kmeans.fit_predict(Xs)

    # scatter for first two features (usually avg_km & fleet_size)
    plt.figure(figsize=(8,6))
    sns.scatterplot(x=fleet_df[features[0]], y=fleet_df[features[1]], hue=fleet_df['cluster'], palette="tab10", s=60)
    plt.xlabel(features[0]); plt.ylabel(features[1])
    plt.title(f"Clusters (k={best_k}) â€” {features[0]} vs {features[1]}")
    plt.tight_layout()
    plt.savefig(os.path.join(OUT_DIR, "clusters_adaptive.png"), dpi=150)
    plt.close()

    # cluster profiling
    agg_cols = {c:["mean","median"] for c in features}
    if fleet_id_col in fleet_df.columns:
        agg_cols[fleet_id_col] = ["count"]
    profile = fleet_df.groupby('cluster').agg(agg_cols)
    # flatten
    profile.columns = ["_".join(map(str, col)).strip() for col in profile.columns.values]
    profile = profile.reset_index()
    profile.to_csv(os.path.join(OUT_DIR, "cluster_profile_adaptive.csv"), index=False)
    fleet_df.to_csv(os.path.join(OUT_DIR, "fleet_with_clusters_adaptive.csv"), index=False)
    print("Saved cluster_profile_adaptive.csv and fleet_with_clusters_adaptive.csv")

# -------------------------
# Step 5: create folium map of zone centroids (if zone column present)
# -------------------------
if zone_col in fleet_df.columns:
    zone_counts = fleet_df[zone_col].value_counts().to_dict()
    # approximate coords for common Hyderabad zones (extend as needed)
    zone_coords = {
        "Gachibowli": (17.4474, 78.3489),
        "Madhapur": (17.4435, 78.3772),
        "Banjara Hills": (17.4065, 78.4691),
        "Abids": (17.3850, 78.4867),
        "Kukatpally": (17.4296, 78.4145),
        "Other": (17.3850, 78.4867)
    }
    m = folium.Map(location=[17.3850, 78.4867], zoom_start=12)
    for z, cnt in zone_counts.items():
        lat, lon = zone_coords.get(z, (17.3850,78.4867))
        folium.CircleMarker(location=(lat,lon), radius=6 + cnt/8,
                            popup=f"{z}: {cnt} fleets", color='blue', fill=True).add_to(m)
    map_path = os.path.join(OUT_DIR, "hyderabad_zone_map_adaptive.html")
    m.save(map_path)
    print("Saved zone map:", map_path)

print("All outputs saved to folder:", OUT_DIR)